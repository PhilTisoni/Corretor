{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulos e Bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, vamos atribuir os módulos e bibliotecas utilizadas no código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd           # Importando Pandas para criação do Dataset\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk      # importando NLTK para utilização do Tokenize\n",
    "nltk.download()\n",
    "from nltk.stem import RSLPStemmer   # Utilizado para Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um Dataset é um conjunto de dados, geralmente, organizados em forma de tabelas. Para essa aplicação, utilizaremos como Dataset alguns verbos em formato txt que poderão ser utlizados na pesquisa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('verbos.txt', header = 0, delim_whitespace=True)  # Abre arquivo txt\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa técnica separa as informações de uma frase em palavras, semelhante ao split utilizado em strings. Em outras palavras, o Tokenize \"quebra\" o texto, dividindo-o em palavras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frase = input('Digite uma frase: ')  # Comando de entrada para o usuário\n",
    "frase = frase.lower()                # Deixa a frase em letras minúsculas\n",
    "tok = nltk.word_tokenize(frase)      # Comando de token\n",
    "\n",
    "print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diminui a palavra para o seu radical, ou seja, faz com que o programa trate a palavra e suas derivações da mesma forma. Um exemplo seria as palavras JOGAR, JOGO ou JOGADA. Elas possuem classes gramaticais diferentes, mas ao aplicar o Stemming, ambas tornam-se JOG (radical). Porém, essa técnica não apresenta bons resultados em textos em Português, sendo necessário a implementação de códigos extensos ou complexos para sua aplicação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = RSLPStemmer()  # Define o Stemming\n",
    "\n",
    "i = len(tok)  # Variável que conta a quantidade de palavras quebradas pelo token\n",
    "x = 0         # Variável para o laço While\n",
    "\n",
    "print(i)      # Imprime a quantidade de palavras\n",
    "print(frase)  # Imprime a frase completa para comparação\n",
    "\n",
    "while x < i:\n",
    "    st = stemmer.stem(tok[x])  # Realiza o Stemming em cada palavra conforme o laço avança\n",
    "    print (st)\n",
    "    x = x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "São palavras que param ou \"atrapalham\" o resumo das palavras e podem ser removidas sem causar prejuízos. Geralmente são artigos muito utilizados como: a, de, o, da, entre outros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')  # Essa é a lista de stopwords em Português fornecidas pelo NLTK\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemovStopWords(instancia):                                 \n",
    "    stopwords = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "    palavras = [i for i in tok if not i in stopwords]\n",
    "    return (\" \".join(palavras))\n",
    "\n",
    "RemovStopWords(frase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
